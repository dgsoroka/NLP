{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pymorphy3\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import umap\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Разложение матрицы на три с сокращением размерности.\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>В МИД Японии заявили, что страна предоставила ...</td>\n",
       "      <td>\\nВласти Японии к настоящему времени предостав...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Белгородская область подверглась новой атаке б...</td>\n",
       "      <td>\\nНочью в среду, 24 мая, в Белгородской област...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Климатолог рассказал о сезоне ураганов в Москве</td>\n",
       "      <td>\\nВ столичном регионе в конце мая – начале июн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Российский холдинг Segezha Group продал европе...</td>\n",
       "      <td>\\nРоссийский лесопромышленный холдинг Segezha ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Жители Москвы могут выбрать цвета, которыми об...</td>\n",
       "      <td>\\nЖители Москвы могут проголосовать за одну из...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>Лидеры G7 договорились продолжить поддержку Ук...</td>\n",
       "      <td>\\nПравительство России расширило доступ произв...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>В кабмине ожидают, что рынок БПЛА в России по ...</td>\n",
       "      <td>\\nЛидеры государств, входящих в \"Большую семер...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>РИА Новости: в России средний размер автокреди...</td>\n",
       "      <td>\\nNikkei: Япония запретит предоставлять России...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>ВСУ обстреляли село Муром Белгородской области</td>\n",
       "      <td>\\nИтальянский ракетный эскадренный миноносец п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>Схема движения автотранспорта временно изменит...</td>\n",
       "      <td>\\nНа юго-востоке Москвы мужчина зашел в поликл...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>408 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title   \n",
       "0    В МИД Японии заявили, что страна предоставила ...  \\\n",
       "1    Белгородская область подверглась новой атаке б...   \n",
       "2      Климатолог рассказал о сезоне ураганов в Москве   \n",
       "3    Российский холдинг Segezha Group продал европе...   \n",
       "4    Жители Москвы могут выбрать цвета, которыми об...   \n",
       "..                                                 ...   \n",
       "403  Лидеры G7 договорились продолжить поддержку Ук...   \n",
       "404  В кабмине ожидают, что рынок БПЛА в России по ...   \n",
       "405  РИА Новости: в России средний размер автокреди...   \n",
       "406     ВСУ обстреляли село Муром Белгородской области   \n",
       "407  Схема движения автотранспорта временно изменит...   \n",
       "\n",
       "                                                  text  \n",
       "0    \\nВласти Японии к настоящему времени предостав...  \n",
       "1    \\nНочью в среду, 24 мая, в Белгородской област...  \n",
       "2    \\nВ столичном регионе в конце мая – начале июн...  \n",
       "3    \\nРоссийский лесопромышленный холдинг Segezha ...  \n",
       "4    \\nЖители Москвы могут проголосовать за одну из...  \n",
       "..                                                 ...  \n",
       "403  \\nПравительство России расширило доступ произв...  \n",
       "404  \\nЛидеры государств, входящих в \"Большую семер...  \n",
       "405  \\nNikkei: Япония запретит предоставлять России...  \n",
       "406  \\nИтальянский ракетный эскадренный миноносец п...  \n",
       "407  \\nНа юго-востоке Москвы мужчина зашел в поликл...  \n",
       "\n",
       "[408 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_texts = pd.DataFrame(pd.read_csv(\"hrefs.CSV\"), columns=['title', 'text'])\n",
    "news_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the SVD step: 14%\n",
      "(408, 20) [[ 0.19211723 -0.05983716 -0.05851599 ...  0.05303783  0.06798314\n",
      "   0.11400188]\n",
      " [ 0.26220213  0.29105885 -0.03566937 ... -0.03496959 -0.02840279\n",
      "   0.03871183]\n",
      " [ 0.15600969 -0.00441021  0.23660962 ... -0.06328228 -0.01617342\n",
      "   0.05322759]\n",
      " ...\n",
      " [ 0.22544302 -0.01240082  0.04857593 ...  0.04296395 -0.03003133\n",
      "  -0.02411102]\n",
      " [ 0.16817652 -0.0306237  -0.01669047 ... -0.00448548  0.02142385\n",
      "  -0.07626606]\n",
      " [ 0.11752255  0.01529366  0.03035909 ... -0.02441485 -0.01273865\n",
      "   0.00630432]]\n",
      "CPU times: total: 312 ms\n",
      "Wall time: 131 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Считаем матрицу термин-документ, но не с частотами, а со значениями Tf*Idf\n",
    "X = TfidfVectorizer().fit_transform(news_texts['text'])\n",
    "# Проводим SVD-разложение по 20 компонентам.\n",
    "svd = TruncatedSVD(n_components=20)\n",
    "X2 = svd.fit_transform(X)\n",
    "\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(f\"Explained variance of the SVD step: {int(explained_variance * 100)}%\")\n",
    "# Просто чтобы посмотреть, что там в самом деле вектора.\n",
    "print(X2.shape, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<408x16600 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 56024 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список значимых частей речи. \n",
    "# Они нам потом понадобятся в немного другом виде. Так что сделаем словарь. чтобы два раза не вставать.\n",
    "conv_pos = {'ADJF':'ADJ', 'ADJS':'ADJ', 'ADV':'ADV', 'NOUN':'NOUN', \n",
    "            'VERB':'VERB', 'PRTF':'ADJ', 'PRTS':'ADJ', 'GRND':'VERB'}\n",
    "\n",
    "tmp_dict = {} # Кеш значимых слов.\n",
    "nones = {} # Кеш незначимых слов.\n",
    "\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "# Фильтруем по части речи и возвращаем только начальную форму.\n",
    "def normalizePymorphy(text, need_pos=True):\n",
    "    tokens = re.findall('[A-Za-zА-Яа-яЁё]+\\-[A-Za-zА-Яа-яЁё]+|[A-Za-zА-Яа-яЁё]+', text)\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        # Если токен уже был закеширован, быстро возьмем результат из него.\n",
    "        if t in tmp_dict.keys():\n",
    "            words.append(tmp_dict[t])\n",
    "        # Аналогично, если он в кеше незначимых слов.\n",
    "        elif t in nones.keys():\n",
    "            pass\n",
    "        # Слово еще не встретилось, будем проводить медленный морфологический анализ.\n",
    "        else:\n",
    "            pv = morph.parse(t)\n",
    "            if pv[0].tag.POS != None:\n",
    "                if pv[0].tag.POS in conv_pos.keys():\n",
    "                    if need_pos:\n",
    "                        word = pv[0].normal_form+\"_\"+conv_pos[pv[0].tag.POS]\n",
    "                    else:\n",
    "                        word = pv[0].normal_form\n",
    "                    # Отправляем слово в результат, ...\n",
    "                    words.append(word)\n",
    "                    # ... и кешируем результат его разбора.\n",
    "                    tmp_dict[t] = word\n",
    "                else:\n",
    "                    # Для незначимых слов можно даже ничего не хранить. Лишь бы потом не обращаться к морфологии.\n",
    "                    nones[t] = \"\"\n",
    "                    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.34 s\n",
      "Wall time: 1.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Приведем слова в текстах к начальным формам.\n",
    "news_texts['NText'] = news_texts['text'].map(lambda x:' '.join(normalizePymorphy(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 78.1 ms\n",
      "Wall time: 85 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "X_l = TfidfVectorizer().fit_transform(news_texts['NText'])\n",
    "svd = TruncatedSVD(n_components=10)\n",
    "X2_l = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовим целевые переменные. Мы же знаем, что там ровно по 1000 текстов для каждой области.\n",
    "classes = np.ones(5000)\n",
    "for i in range(2, 6):\n",
    "    classes[(i-1)*1000: i*1000] = i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
