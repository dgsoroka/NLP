{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pymorphy3\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import umap\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Разложение матрицы на три с сокращением размерности.\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Калашников\" выпустит партию обновленных АК-12...</td>\n",
       "      <td>\\n                            Во втором полуго...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Калашников\" начал поставлять снайперскую винт...</td>\n",
       "      <td>\\n                            Поставки новейше...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Черный май\": как немецкие подводники потерпел...</td>\n",
       "      <td>\\n                            24 мая исполняет...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В РФ продолжают испытывать новый ракетный комп...</td>\n",
       "      <td>\\n                            В России продолж...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>За что командир советской подлодки \"Щ-408\" пол...</td>\n",
       "      <td>\\n                            22 мая исполняет...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>ШААЗ выпустил первый отечественный 14-тонный п...</td>\n",
       "      <td>\\n                            «Шадринский авто...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>Деятельность «Ростерминалугля» признана эколог...</td>\n",
       "      <td>\\n                            «Ростерминалугол...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>«Перспектива» для заслуженного отдыха</td>\n",
       "      <td>\\n                            Почему у нашего ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>«Кузбассразрезуголь» создал первый профстандар...</td>\n",
       "      <td>\\n                            «Кузбассразрезуг...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>Локомотивный парк «Кузбассразрезугля» пополнил...</td>\n",
       "      <td>\\n                            «Кузбассразрезуг...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1775 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title   \n",
       "0     \"Калашников\" выпустит партию обновленных АК-12...  \\\n",
       "1     \"Калашников\" начал поставлять снайперскую винт...   \n",
       "2     \"Черный май\": как немецкие подводники потерпел...   \n",
       "3     В РФ продолжают испытывать новый ракетный комп...   \n",
       "4     За что командир советской подлодки \"Щ-408\" пол...   \n",
       "...                                                 ...   \n",
       "1770  ШААЗ выпустил первый отечественный 14-тонный п...   \n",
       "1771  Деятельность «Ростерминалугля» признана эколог...   \n",
       "1772              «Перспектива» для заслуженного отдыха   \n",
       "1773  «Кузбассразрезуголь» создал первый профстандар...   \n",
       "1774  Локомотивный парк «Кузбассразрезугля» пополнил...   \n",
       "\n",
       "                                                   text  \n",
       "0     \\n                            Во втором полуго...  \n",
       "1     \\n                            Поставки новейше...  \n",
       "2     \\n                            24 мая исполняет...  \n",
       "3     \\n                            В России продолж...  \n",
       "4     \\n                            22 мая исполняет...  \n",
       "...                                                 ...  \n",
       "1770  \\n                            «Шадринский авто...  \n",
       "1771  \\n                            «Ростерминалугол...  \n",
       "1772  \\n                            Почему у нашего ...  \n",
       "1773  \\n                            «Кузбассразрезуг...  \n",
       "1774  \\n                            «Кузбассразрезуг...  \n",
       "\n",
       "[1775 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_texts = pd.DataFrame(pd.read_csv(\"news.CSV\"), columns=['title', 'text'])\n",
    "news_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0.0\n",
       "4       0.0\n",
       "       ... \n",
       "1770    3.0\n",
       "1771    3.0\n",
       "1772    3.0\n",
       "1773    3.0\n",
       "1774    3.0\n",
       "Name: theme, Length: 1775, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = pd.Series(pd.read_csv('news.CSV')['theme'])\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the SVD step: 8%\n",
      "(1775, 20) [[ 1.18273873e-01 -1.48476552e-02  4.42085260e-02 ... -1.42706390e-02\n",
      "  -1.59824116e-02  6.78053817e-05]\n",
      " [ 1.38709574e-01 -2.92026518e-02  6.95658865e-02 ... -5.50139314e-03\n",
      "  -3.95390821e-02 -2.47727999e-02]\n",
      " [ 2.58704440e-01 -1.16852813e-01 -3.77281416e-02 ...  8.72860868e-02\n",
      "  -2.36619761e-02 -4.62952337e-02]\n",
      " ...\n",
      " [ 2.02878586e-01  1.53843086e-02 -2.82683305e-02 ...  1.79963909e-02\n",
      "   1.65585093e-03 -3.05940286e-02]\n",
      " [ 2.91129457e-01  3.55962539e-01  2.03465035e-02 ... -1.40852344e-02\n",
      "   4.54465014e-02 -1.00629148e-01]\n",
      " [ 2.88926067e-01  3.41501725e-01  4.57943518e-03 ...  1.14079449e-02\n",
      "  -9.31827430e-03  4.84615604e-03]]\n",
      "CPU times: total: 1.22 s\n",
      "Wall time: 1.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Считаем матрицу термин-документ, но не с частотами, а со значениями Tf*Idf\n",
    "X = TfidfVectorizer().fit_transform(news_texts['text'])\n",
    "# Проводим SVD-разложение по 20 компонентам.\n",
    "svd = TruncatedSVD(n_components=20)\n",
    "X2 = svd.fit_transform(X)\n",
    "\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(f\"Explained variance of the SVD step: {int(explained_variance * 100)}%\")\n",
    "# Просто чтобы посмотреть, что там в самом деле вектора.\n",
    "print(X2.shape, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1775x76252 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 487020 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список значимых частей речи. \n",
    "# Они нам потом понадобятся в немного другом виде. Так что сделаем словарь. чтобы два раза не вставать.\n",
    "conv_pos = {'ADJF':'ADJ', 'ADJS':'ADJ', 'ADV':'ADV', 'NOUN':'NOUN', \n",
    "            'VERB':'VERB', 'PRTF':'ADJ', 'PRTS':'ADJ', 'GRND':'VERB'}\n",
    "\n",
    "tmp_dict = {} # Кеш значимых слов.\n",
    "nones = {} # Кеш незначимых слов.\n",
    "\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "# Фильтруем по части речи и возвращаем только начальную форму.\n",
    "def normalizePymorphy(text, need_pos=True):\n",
    "    tokens = re.findall('[A-Za-zА-Яа-яЁё]+\\-[A-Za-zА-Яа-яЁё]+|[A-Za-zА-Яа-яЁё]+', text)\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        # Если токен уже был закеширован, быстро возьмем результат из него.\n",
    "        if t in tmp_dict.keys():\n",
    "            words.append(tmp_dict[t])\n",
    "        # Аналогично, если он в кеше незначимых слов.\n",
    "        elif t in nones.keys():\n",
    "            pass\n",
    "        # Слово еще не встретилось, будем проводить медленный морфологический анализ.\n",
    "        else:\n",
    "            pv = morph.parse(t)\n",
    "            if pv[0].tag.POS != None:\n",
    "                if pv[0].tag.POS in conv_pos.keys():\n",
    "                    if need_pos:\n",
    "                        word = pv[0].normal_form+\"_\"+conv_pos[pv[0].tag.POS]\n",
    "                    else:\n",
    "                        word = pv[0].normal_form\n",
    "                    # Отправляем слово в результат, ...\n",
    "                    words.append(word)\n",
    "                    # ... и кешируем результат его разбора.\n",
    "                    tmp_dict[t] = word\n",
    "                else:\n",
    "                    # Для незначимых слов можно даже ничего не хранить. Лишь бы потом не обращаться к морфологии.\n",
    "                    nones[t] = \"\"\n",
    "                    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7.09 s\n",
      "Wall time: 7.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Приведем слова в текстах к начальным формам.\n",
    "news_texts['NText'] = news_texts['text'].map(lambda x:' '.join(normalizePymorphy(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 672 ms\n",
      "Wall time: 629 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "X_l = TfidfVectorizer().fit_transform(news_texts['NText'])\n",
    "svd = TruncatedSVD(n_components=10)\n",
    "X2_l = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 3., 3., 3.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = np.array(targets)\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_texts(data, target):\n",
    "    # Делим данные на обучающую и проверочную выборки.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=333)\n",
    "    #Обучаем классификатор и оцениваем точность результатов.\n",
    "    tree = RandomForestClassifier(criterion='entropy', random_state=333)\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_hat = tree.predict(X_test)\n",
    "    print(f\"accuracy = {accuracy_score(y_hat, y_test)}\")\n",
    "    print(confusion_matrix(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9464788732394366\n",
      "[[ 95   9   0   0]\n",
      " [  0 100   1   1]\n",
      " [  3   1  48   2]\n",
      " [  0   2   0  93]]\n"
     ]
    }
   ],
   "source": [
    "classify_texts(X, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.952112676056338\n",
      "[[104   0   0   0]\n",
      " [  1  99   1   1]\n",
      " [  4   6  42   2]\n",
      " [  0   2   0  93]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "# TF*Idf с лемматизацией.\n",
    "classify_texts(X_l, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.971830985915493\n",
      "[[103   1   0   0]\n",
      " [  2  98   1   1]\n",
      " [  2   2  50   0]\n",
      " [  0   1   0  94]]\n",
      "CPU times: total: 484 ms\n",
      "Wall time: 482 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SVD без лемматизации.\n",
    "classify_texts(X2, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9690140845070423\n",
      "[[103   1   0   0]\n",
      " [  1  99   1   1]\n",
      " [  4   2  48   0]\n",
      " [  0   1   0  94]]\n",
      "CPU times: total: 344 ms\n",
      "Wall time: 370 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SVD с лемматизацией.\n",
    "classify_texts(X2_l, targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
